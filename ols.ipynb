{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "import time\n",
    "import pickle\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#***IMPLEMENTATION OF ONLINE LINEARIZED LASSO***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean square error for training data set= 1.636621799208212\n",
      "mean square error for test data set= 1.3537542666433702\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    " # we will use it for the initial batch for convenience\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Online_Linearized_Lasso_for_single_datapoint_addition(): #when the t th data point is added \n",
    "    \n",
    "    #initiating the hyperparameters\n",
    "    \n",
    "    def init(self,learning_rate,no_of_iterations, lambda_parameter,weight,beta_tilda,total_weight,X_new,Y_new):\n",
    "        \n",
    "        self.learning_rate=learning_rate\n",
    "        self.no_of_iterations=no_of_iterations\n",
    "        self.lambda_parameter=lambda_parameter \n",
    "        self.weight=weight #a list of T weights. This is the t-independent case. Similarly we can do for t-dependent case by maling this a parameter of fit() function\n",
    "        self.beta_tilda=beta_tilda\n",
    "        self.total_weight=total_weight # this will depend on t\n",
    "        self.X_new=X_new\n",
    "        self.Y_new=Y_new\n",
    "        \n",
    "    #fitting the dataset to the regression model\n",
    "        \n",
    "    def fit(self,X_initial,Y_initial,X_new_col,Y_new_col):\n",
    "\n",
    "    # m is the total no of data points of initial batch size\n",
    "\n",
    "    # p is the total no of data points of the new collected batch size\n",
    "\n",
    "    # n is the total no of dependent variables\n",
    "    \n",
    "        self.p=len(X_new_col)\n",
    "        self.m = len(X_initial)\n",
    "        self.n = len(X_initial[0])\n",
    "        self.w = [0 for i in range(self.n)]\n",
    "        \n",
    "        self.X_initial=X_initial\n",
    "        self.Y_initial=Y_initial\n",
    "        self.X_new_col=X_new_col\n",
    "        self.Y_new_col=Y_new_col\n",
    "        \n",
    "        \n",
    "\n",
    "    #implementing gradient descent algorithm for optimisation\n",
    "\n",
    "        for i in range(self.no_of_iterations):\n",
    "            self.update_weights()\n",
    "\n",
    "        return(self.w)    \n",
    "            \n",
    "        \n",
    "    #function for updating the weight and bias value\n",
    "        \n",
    "    def update_weights(self):\n",
    "        Y_prediction = [0 for i in range(self.m)]\n",
    "        Y_prediction_new = [0 for i in range(self.m)]\n",
    "        for i in range(self.m):\n",
    "             \n",
    "             Y_prediction[i] = self.predict(self.X_initial[i],self.w)\n",
    "        for i in range(self.m):     \n",
    "             Y_prediction_new[i] = self.predict(self.X_initial[i],self.beta_tilda)\n",
    "             \n",
    "         #hut   \n",
    "             \n",
    "        dw=[0 for i in range(self.n)]\n",
    "        \n",
    "        for i in range(self.n):\n",
    "             c=0\n",
    "             d=0\n",
    "             for j in range(self.m):\n",
    "                 c = c+(Y_prediction[j]-self.Y_initial[j])*self.X_initial[j][i]\n",
    "                \n",
    "                 d = d+(Y_prediction_new[j]-self.Y_initial[j])*self.X_initial[j][i]\n",
    "\n",
    "             b=0\n",
    "             \n",
    "             for j in range(self.p):\n",
    "                 b = b + (self.predict(self.X_new_col[j],self.w) - self.Y_new_col[j])*self.X_new_col[j][i]*(self.weight[j]/self.total_weight)           \n",
    "                 \n",
    "             if self.w[i]>0:\n",
    "                 dw[i]=((c+self.lambda_parameter-d)/self.m)+b\n",
    "                 \n",
    "             else:\n",
    "                 dw[i]=((c-self.lambda_parameter-d)/self.m)+b        \n",
    "                              \n",
    "             \n",
    "                    \n",
    "                \n",
    "\n",
    "\n",
    "        for i in range(self.n):\n",
    "\n",
    "            self.w[i] = self.w[i]-self.learning_rate*dw[i]\n",
    "            \n",
    "\n",
    "                \n",
    "        \n",
    "        \n",
    "    #predicting the target variable\n",
    "        \n",
    "    def predict(self,X,w):\n",
    "\n",
    "        s=0\n",
    "        for i in range(len(w)):\n",
    "            s=s+X[i]*w[i]\n",
    "        return s\n",
    "\n",
    "    def mean_squared_error(self,X,Y,w):\n",
    "        s=0\n",
    "        for i in range(len(X)):\n",
    "            s=s + (Y[i] - self.predict(X[i],w))**2\n",
    "        return (s/len(X))       \n",
    "\n",
    "def OnlineLinearisedLASSO(X,Y,X_new,Y_new,learning_rate,no_of_iterations, lambda_parameter,weight):\n",
    "    l=Online_Linearized_Lasso_for_single_datapoint_addition()\n",
    "    ls=linear_model.Lasso(alpha=lambda_parameter[0]) #lamba parameter is a list with T+1 elements T = len(X_new)\n",
    "    ls.fit(X,Y)\n",
    "    beta_tilda=ls.coef_\n",
    "    for i in range(len(Y_new)):\n",
    "            \n",
    "        total_weight=1\n",
    "        for j in range(i):\n",
    "            total_weight = total_weight + weight[j]\n",
    "        l.init(learning_rate,no_of_iterations, lambda_parameter[i+1],weight,beta_tilda,total_weight,X_new[:i+1],Y_new[:i+1])\n",
    "        beta_tilda=l.fit(X_initial=X,Y_initial=Y,X_new_col=X_new,Y_new_col=Y_new)\n",
    "    return (beta_tilda)    \n",
    "        \n",
    "theta_true = 0.5*scipy.sparse.random(50, 1, density=0.04,random_state=np.random.default_rng()).A.reshape(50)\n",
    "X= np.random.normal(0,1,(50,50))\n",
    "Y = X.T.dot(theta_true) + np.random.normal(0,1,50)\n",
    "X_new = np.random.normal(0,1,(50,50))\n",
    "Y_new= X_new.T.dot(theta_true) + np.random.normal(0,1,50)\n",
    "lambda_parameter=[25 for i in range(51)]\n",
    "weight=[1 for i in range(50)]\n",
    "beta_star=OnlineLinearisedLASSO(X=X,Y=Y,X_new=X_new,Y_new=Y_new,learning_rate=0.01,no_of_iterations=100, lambda_parameter=lambda_parameter,weight=weight)\n",
    "l=Online_Linearized_Lasso_for_single_datapoint_addition()\n",
    "print(\"mean square error for training data set=\", l.mean_squared_error(X,Y,beta_star))\n",
    "print(\"mean square error for test data set=\", l.mean_squared_error(X_new,Y_new,beta_star))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
